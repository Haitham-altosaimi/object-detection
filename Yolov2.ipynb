{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_3hOKSVI2cHe"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Yolov2(nn.Module):\n","  def __init__(self, num_classes=1):\n","    super(Yolov2, self).__init__()\n","    self.backbone = nn.Sequential(\n","        #conv1\n","        nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(32),\n","        nn.LeakyReLU(inplace=True),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        #conv2\n","        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(inplace=True),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        #conv3\n","        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(128),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv4\n","        nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0),\n","        nn.BatchNorm2d(64),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv5\n","        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(128),\n","        nn.LeakyReLU(inplace=True),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        #conv6\n","        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(256),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv7\n","        nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0),\n","        nn.BatchNorm2d(128),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv8\n","        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(256),\n","        nn.LeakyReLU(inplace=True),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        #conv9\n","        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv10\n","        nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0),\n","        nn.BatchNorm2d(256),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv11\n","        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv12\n","        nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0),\n","        nn.BatchNorm2d(256),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv13\n","        nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.LeakyReLU(inplace=True),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        #conv14\n","        nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(1024),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv15\n","        nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv16\n","        nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(1024),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv17\n","        nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=1),\n","        nn.BatchNorm2d(512),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv18\n","        nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n","        nn.BatchNorm2d(1024),\n","        nn.LeakyReLU(inplace=True),\n","\n","        #conv19\n","        nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1),\n","        #nn.BatchNorm2d(1024),\n","        #nn.LeakyReLU(inplace=True),\n","\n","        #conv20\n","        nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=0),\n","        #nn.BatchNorm2d(1024),\n","        #nn.LeakyReLU(inplace=True),\n","\n","        #conv21\n","        nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=0),\n","        #nn.BatchNorm2d(1024),\n","        #nn.LeakyReLU(inplace=True),\n","\n","    )\n","    self.final_layer = nn.Conv2d(3072, 30, kernel_size=1, stride=1, padding=0)\n","    self.avg = nn.AvgPool2d((1,1))\n","\n","  def forward(self, x):\n","    for i, layer in enumerate(self.backbone):\n","\n","        x = layer(x)\n","        if i == 40:\n","\n","           passthrough_layer = torch.cat( (x[:,:, 0:13,0:13] , x[:,:,0:13,13:26],\n","                              x[:,:,13:26,0:13], x[:,:,13:26,13:26]) , dim=1)\n","\n","    x = torch.cat((passthrough_layer, x), dim=1)\n","    x = self.final_layer(x)\n","    x = self.avg(x)\n","    x = x.permute(0, 2, 3, 1)\n","\n","\n","    return x\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1688914623159,"user":{"displayName":"HAITHAM AL-TOSAIMI","userId":"14079608149427840165"},"user_tz":-120},"id":"npMLVuEZJSad","outputId":"3bee1141-af8f-4968-e50a-85be149fa9e3"},"outputs":[{"data":{"text/plain":["tensor(nan)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.log(torch.tensor(-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyGGU32ajw-4"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","\n","\n","class yolov2_main(nn.Module):\n","  def __init__(self, yolov2=None):\n","    super(yolov2_main, self).__init__()\n","    self.backbone = yolov2()\n","\n","  def forward(self, images, targets=None, anchors=None):\n","\n","    predictions = self.backbone(images)\n","    loss = self.calc_loss(predictions, targets, images, anchors)\n","\n","    return loss\n","\n","  def calc_loss(self, predictions, targets, images, anchors):\n","\n","      ##Note:::: this model has no classifier for the reason that it addresses face detection, so\n","      ### objectness score is the major element\n","\n","      pre_processing = output_preprocessing(predictions, targets, images, anchors)\n","\n","      target_offsets = pre_processing[0]\n","      pred_offsets = pre_processing[1][:,0:4]\n","\n","\n","      loss_xy = F.mse_loss(nn.functional.sigmoid(pred_offsets[:,:2]), target_offsets[:,:2], reduction='sum')\n","      loss_wh = F.mse_loss(torch.exp(pred_offsets[:,2:4]), target_offsets[:,2:4], reduction='sum')\n","      offset_loss = (loss_xy + loss_wh )/target_offsets[:,0:4].numel() * 2\n","\n","\n","      neg_conf_scores = pre_processing[2]\n","      neg_conf_target = torch.zeros_like(neg_conf_scores).to(device=neg_conf_scores.device)\n","      negative_conf_loss = F.mse_loss(nn.functional.sigmoid(neg_conf_scores), neg_conf_target, reduction='sum')/neg_conf_target.numel() * 0.6\n","\n","\n","\n","      pos_conf_scores = pre_processing[1][:, 4]\n","      pos_conf_target = torch.ones_like(pos_conf_scores,dtype=pos_conf_scores.dtype).to(device=pos_conf_scores.device)\n","      positive_conf_loss = F.mse_loss(nn.functional.sigmoid(pos_conf_scores), pos_conf_target, reduction='sum')/pos_conf_target.numel() * 0.5\n","\n","\n","      return (offset_loss + negative_conf_loss + positive_conf_loss), offset_loss, negative_conf_loss, positive_conf_loss\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ei9cjOHRY0Ij"},"outputs":[],"source":["from tqdm import tqdm\n","from functools import partial\n","tqdm = partial(tqdm, position=0, leave=True)\n","import torch\n","import numpy as np\n","import time\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","\n","def train_epocs(model, optimizer, data_loader, devie, all_anchors ,epochs=30,training_state=False ):\n","\n","    for epoch in range(epochs):\n","        if epoch==30:\n","           parameters = model.parameters()\n","           optimizer = torch.optim.SGD(parameters, lr = 0.000002,\n","                                       momentum=0.99)\n","        total = 0\n","        sum_loss = 0\n","        sum_loss_classifier = 0\n","        sum_loss_offsets = 0\n","        sum_loss_noobj = 0\n","        sum_loss_obj = 0\n","\n","        iteration_num = 0\n","\n","        for data in tqdm(data_loader):\n","            optimizer.zero_grad()\n","            images = data[0].to(device)\n","            #targets = [ boxes[2].to(device) for boxes in data[1]]\n","            #targets = torch.stack(targets)\n","            targets = data[1]\n","            batch_length = len(images)\n","            ### gradient tracking\n","            if not training_state:\n","               with torch.no_grad():\n","                  model.eval()\n","                  output = model(images, targets = targets, anchors = all_anchors)\n","            else:\n","                 model.train()\n","                 output = model(images, targets, all_anchors)\n","            Final_loss = output[0]\n","            if training_state:\n","               optimizer.zero_grad()\n","               Final_loss.backward()\n","               optimizer.step()\n","            iteration_num += 1\n","            total += batch_length\n","\n","            sum_loss += Final_loss\n","            #sum_loss_classifier += output[1]\n","            sum_loss_offsets += output[1]\n","            sum_loss_noobj += output[2]\n","            sum_loss_obj += output[3]\n","\n","            if iteration_num % 50 == 0:\n","\n","                train_loss = sum_loss/total\n","                train_loss_classifier = sum_loss_classifier/total\n","                train_loss_offsets = sum_loss_offsets/total\n","                train_loss_noobj = sum_loss_noobj/total\n","                train_loss_obj = sum_loss_obj/total\n","                print(\"loss %.6f classifer %.6f regressor %.6f noobj_loss %.6f obj_loss %.6f \"%\n","                      (train_loss, train_loss_classifier, train_loss_offsets, train_loss_noobj, train_loss_obj))\n","                total = 0.0\n","                sum_loss= 0.0\n","                sum_loss_classifier = 0.0\n","                sum_loss_offsets = 0.0\n","                sum_loss_noobj = 0.0\n","                sum_loss_obj = 0.0\n","\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1335,"status":"ok","timestamp":1689374172952,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"QH1IuCwB7SQ9","outputId":"b0516745-2e94-4725-ff94-b9e47831deb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = yolov2_main(yolov2=Yolov2).to(device)\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-4 , momentum = 0.99)\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZC4uFmA8Zat9"},"outputs":[],"source":["torch.s ave(model.state_dict(), '/content/gdrive/MyDrive/yolov2new.pth')\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10775,"status":"ok","timestamp":1689374184260,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"70vjiGD-oNLd","outputId":"f5b5e03b-59f3-467b-cc54-113d9c094801"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}],"source":["model.load_state_dict(torch.load('/content/gdrive/MyDrive/yolov2new.pth', map_location=device))\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tF153O-u6_6p"},"outputs":[],"source":["train = train_epocs(model, optimizer, data_loader, device, all_anchors,epochs=1 ,training_state=True)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6lzgjTVbeVz"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","def output_preprocessing(predictions, gt_boxes, images, anchors):\n","  target_offsets = []\n","  pred_offsets =[]\n","  neg_objectness_scores = []\n","  neg_objectness_scores_within = []\n","\n","\n","  for pred, gt in zip(predictions, gt_boxes):\n","    centroids = torch.tensor([ sample[1] for sample in gt ]).to(device=pred.device)\n","    boxes_in_image = torch.stack([ sample[2] for sample in gt]).to(device=pred.device)\n","\n","\n","    grid_cell_mask = torch.zeros((13,13)).to(device=pred.device)\n","    grid_cell_mask[centroids[:,1], centroids[:,0]] = 1\n","    mask_locations = torch.where(grid_cell_mask==0)\n","    masked_values = pred[ mask_locations[0],mask_locations[1],:]\n","    scores_locations = [4, 10, 16, 22, 28]\n","\n","    #check_predicted_boxes = check(pred.detach().clone().to(device=torch.device('cpu')), gt,\n","                                  images[0].detach().clone().to(device=torch.device('cpu')), anchors)\n","    neg_objectness_scores.append(masked_values[:,scores_locations].view(-1))\n","\n","\n","\n","    unique_centroids, counts = torch.unique(centroids, return_counts=True, dim=0)\n","\n","\n","    for grid_cell in unique_centroids:\n","        indices_of_boxes_in_cell_mask = torch.all(centroids==grid_cell, dim=1)\n","        indices_of_boxes_in_cell = torch.where(indices_of_boxes_in_cell_mask)[0]\n","\n","\n","        boxes_in_cell   = boxes_in_image[indices_of_boxes_in_cell]\n","\n","\n","\n","        pred_boxes_in_cell = pred[ grid_cell[0], grid_cell[1],:].reshape(-1,6)\n","\n","        anchors_in_cell = torch.tensor(anchors[ (grid_cell[0]*13) + grid_cell[1] ]).to(device=pred.device)\n","\n","        assigned_anchors, assigned_bboxes, anchor_indices = anchors_assignmets( anchors_in_cell, boxes_in_cell )\n","        '''\n","        img = images[0].to(torch.device('cpu')).numpy().transpose(1,2,0).copy()\n","        for d in assigned_anchors:\n","          x1 = int(d[0])\n","          y1 = int(d[1])\n","          x2 = int(d[2])\n","          y2 = int(d[3])\n","          cv2.rectangle(img, (x1,y1), (x2,y2),(0,0,255), 2)\n","\n","        for d in assigned_bboxes:\n","          x1 = int(d[0] - d[2]/2 )\n","          y1 = int(d[1] - d[3]/2)\n","          x2 = int(d[2]/2 + d[0])\n","          y2 = int(d[3]/2 + d[1] )\n","          cv2.rectangle(img,(x1, y1), (x2, y2), (255,0,0), 1)\n","        plt.imshow(img)\n","        print(er)\n","        '''\n","        assigned_pred = pred_boxes_in_cell[anchor_indices,:]\n","\n","        for i in range(anchors_in_cell.size(0)):\n","            if i not in anchor_indices:\n","               neg_objectness_scores_within.append(pred_boxes_in_cell[i][-2])\n","\n","        target_bbox = target_calculation(assigned_anchors, assigned_bboxes)\n","\n","        target_offsets.append(torch.stack(target_bbox))\n","        pred_offsets.append(assigned_pred)\n","\n","\n","  negative_scores = torch.cat( (torch.cat(neg_objectness_scores), torch.stack(neg_objectness_scores_within)) )\n","\n","  return torch.cat(target_offsets), torch.cat(pred_offsets).view(-1,6), negative_scores\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ku_w5Kbz61EF"},"outputs":[],"source":["def target_calculation(assigned_anchors, assigned_bbox):\n","    target_boxes = []\n","    for anchor, box in zip(assigned_anchors, assigned_bbox):\n","        x = (box[0] - ( int(box[0]/32) * 32))/32\n","        y = (box[1] - ( int(box[1]/32) * 32))/32\n","\n","        anchor_w = anchor[2] - anchor[0]\n","        anchor_h = anchor[3] - anchor[1]\n","\n","        w = box[2]/anchor_w\n","        h = box[3]/anchor_h\n","\n","        target = torch.tensor([x, y, w, h], dtype=torch.float32).to(box.device)\n","        target_boxes.append(target)\n","\n","    return target_boxes\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BShMgDlxn8Cp"},"outputs":[],"source":["def calc_iou(anchors, gt_boxes):\n","  anchors_area = (anchors[:,2] - anchors[:,0]) * (anchors[:,3]-anchors[:,1] )\n","  boxes_area = (gt_boxes[:,2]) * (gt_boxes[:,3])\n","  upper_left_intersection = torch.max(anchors[:,:2].unsqueeze(1), (gt_boxes[:,:2] - gt_boxes[:,2:]/2))\n","  lower_right_intersection = torch.min(anchors[:, 2:].unsqueeze(1), (gt_boxes[:,:2] + gt_boxes[:,2:]/2))\n","  intersection_dimension = (lower_right_intersection - upper_left_intersection).clamp(min=0)\n","  intersection_area = intersection_dimension[:,:,0] * intersection_dimension[:,:,1]\n","  union_area = anchors_area.unsqueeze(1) + boxes_area - intersection_area\n","  return intersection_area/union_area\n","\n","def anchors_assignmets(anchors, gt):\n","    assigned_anchors = []\n","    assigned_bboxes = []\n","    anchors_indices = []\n","\n","    if len(anchors) < len(gt):\n","       gt = gt[: len(anchors)]\n","\n","    iou = calc_iou(anchors, gt)\n","    iou = torch.transpose(iou, 0, 1)\n","\n","    for i, col in enumerate(iou):\n","        trig = 0\n","        values, indices = torch.sort(col, descending=True)\n","        for num in range(len(anchors_indices)):\n","          if indices[num] in anchors_indices:\n","            trig = num+1\n","          else:\n","            break\n","        anchors_indices.append(indices[trig])\n","        assigned_bboxes.append(gt[i])\n","        assigned_anchors.append(anchors[indices[trig]])\n","\n","    return torch.stack(assigned_anchors), torch.stack(assigned_bboxes), anchors_indices\n","\n","\n"]},{"cell_type":"code","source":["import torch\n","\n","x = torch.arange(0,416,32)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"332GQfGGu_Td","executionInfo":{"status":"ok","timestamp":1689461307747,"user_tz":-120,"elapsed":4648,"user":{"displayName":"HAITHAM AL-TOSAIMI","userId":"14079608149427840165"}},"outputId":"fec98d7e-ecee-41bb-ebb0-d8220f899503"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([  0,  32,  64,  96, 128, 160, 192, 224, 256, 288, 320, 352, 384])\n"]}]},{"cell_type":"code","source":["from matplotlib.patches import BoxStyle\n","def check(pred, gt, image, anchors):\n","  pred = pred.view(13,13,5,6)\n","  prediction = convert_offsets_pred(pred, anchors)\n","  boxes_with_boxes = torch.where( nn.functional.sigmoid(pred[:,:,:,4]) > 0.8 )\n","  true_predictions = prediction[boxes_with_boxes]\n","\n","\n","  img = image.numpy().transpose(1,2,0).copy()\n","  for d in true_predictions:\n","    x1 = int( d[0] -d[2]/2 )\n","    y1 = int( d[1]- d[3]/2 )\n","    x2 = int( d[2]/2 + d[0])\n","    y2 = int( d[3]/2 + d[1])\n","    cv2.rectangle(img ,(x1, y1),(x2, y2), (255,0,0), 2)\n","  plt.imshow(img)\n","  print(nm)\n","\n","\n","\n","\n"],"metadata":{"id":"ZKwua3JLPH88"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiOkOX_1xmOC"},"outputs":[],"source":["\n","def convert_offsets_pred(pred, anchors):\n","  c = torch.arange(0,416,32)\n","  cx, cy = torch.meshgrid(c,c)\n","\n","  bx = nn.functional.sigmoid(pred[:,:,:, 0])*32 + cx.unsqueeze(-1)\n","  by = nn.functional.sigmoid(pred[:,:,:, 1])*32 + cy.unsqueeze(-1)\n","\n","  anchors = torch.tensor(anchors).view(13,13,5,-1)\n","  anchors[:,:,:,2] = anchors[:,:,:,2] - anchors[:,:,:,0]\n","  anchors[:,:,:,3] = anchors[:,:,:,3] - anchors[:,:,:,1]\n","\n","  bw = anchors[:,:,:,2] * torch.exp(pred[:,:,:,2])\n","  bh = anchors[:,:,:,3] * torch.exp(pred[:,:,:,3])\n","\n","\n","  offsets = torch.cat((bx.unsqueeze(-1), by.unsqueeze(-1), bw.unsqueeze(-1), bh.unsqueeze(-1)), dim=-1)\n","\n","  return offsets\n","\n","\n","\n"]},{"cell_type":"markdown","source":["Data uploading"],"metadata":{"id":"2AEh84i3v8Cz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15793,"status":"ok","timestamp":1689374000406,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"rGhoaf7hmVGO","outputId":"78996d48-5068-4f70-9e9a-e155c8357341"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":911,"status":"ok","timestamp":1689374004770,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"KislBHadkeyj","outputId":"baed1d60-a172-4f18-c32d-e2f228aacec0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/gdrive/MyDrive/wider_face_split.zip\n","   creating: wider_face_split/\n","  inflating: wider_face_split/readme.txt  \n","  inflating: wider_face_split/wider_face_test.mat  \n","  inflating: wider_face_split/wider_face_test_filelist.txt  \n","  inflating: wider_face_split/wider_face_train.mat  \n","  inflating: wider_face_split/wider_face_train_bbx_gt.txt  \n","  inflating: wider_face_split/wider_face_val.mat  \n","  inflating: wider_face_split/wider_face_val_bbx_gt.txt  \n"]}],"source":["!unzip /content/gdrive/MyDrive/wider_face_split.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZ3nU_tskfLi"},"outputs":[],"source":["!unzip /content/gdrive/MyDrive/WIDER_train.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q76CZjaGaj7Z"},"outputs":[],"source":["\n","from PIL import Image\n","import torchvision\n","import numpy as np\n","import cv2\n","import torch\n","\n","class LoadDataSet(torch.utils.data.Dataset):\n","  def __init__(self, annotation_path, images_path, transforms):\n","    self.images_path_list = images_path\n","    self.annotation_path_list = annotation_path\n","    self.transform = transforms\n","    self.size = len(self.images_path_list)\n","    print(self.size)\n","\n","  def __getitem__(self, idx):\n","    Transform = torchvision.transforms.Normalize(mean=[0.485,0.456,0.406],\n","                                                 std=[0.229,0.224,0.225])\n","\n","    img_path = '/content/WIDER_train/images/'+self.images_path_list[idx]\n","    gt_boxes = self.annotation_path_list[idx]\n","    img = Image.open(img_path)\n","    img = np.asarray(img)\n","    img_size = img.shape\n","    image = cv2.resize(img/255., (416,416), interpolation=cv2.INTER_AREA)\n","    image = torch.as_tensor(image, dtype=torch.float32).permute(2,0,1)\n","    image = Transform(image)\n","    scalar = torch.tensor([1, 416/img_size[1], 416/img_size[0], 416/img_size[1], 416/img_size[0]], dtype=torch.float32)\n","    boxes = []\n","    scale_ratio = 32\n","\n","    for box in gt_boxes:\n","        s_box = torch.tensor(box, dtype=torch.float32)*scalar\n","\n","        x_ctr = (s_box[3] + s_box[1])/2\n","        w     = (s_box[3] - s_box[1])\n","        #print(\"x_ctr\", x_ctr)\n","        x0_cell = int(x_ctr/scale_ratio)\n","\n","        y_ctr = (s_box[4] + s_box[2])/2\n","        h     = (s_box[4] - s_box[2])\n","        #print(\"y_ctr\", y_ctr)\n","        y0_cell = int(y_ctr/scale_ratio)\n","        '''\n","        if (x_ctr, y_ctr) in intersection and (y_ctr==0 or x_ctr==0):\n","          print(x_ctr, y_ctr)\n","          print(img_path)\n","        intersection.append((x_ctr, y_ctr))\n","        '''\n","        s_box[1] = x_ctr\n","        s_box[2] = y_ctr\n","        s_box[3] = w\n","        s_box[4] = h\n","\n","        boxes.append([1, (x0_cell, y0_cell), s_box[1:]])\n","\n","    return image, boxes, img_path\n","\n","  def __len__(self):\n","    return self.size\n","\n","def collate_fn(batch):\n","  images = []\n","  boxes = []\n","  for data in batch:\n","    images.append(data[0])\n","    boxes.append(data[1])\n","  return torch.stack(images, dim=0), boxes, data[2]\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ttp1vBLV9pGg"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uASR-uX0k-FT"},"outputs":[],"source":["file = open('/content/wider_face_split/wider_face_train_bbx_gt.txt')\n","lista=[]\n","inside_list=[]\n","names=[]\n","# this parameter is used to generate the anchors\n","all_boxes = []\n","triger=0\n","\n","for idx, line in enumerate(file.readlines()):\n","    if line[-4:]==\"jpg\\n\":\n","       names.append(line[:-1])\n","       if (inside_list==[] or triger==1) and idx!=0:\n","          names.pop(len(names)-2)\n","          inside_list = []\n","          triger = 0\n","          continue\n","       if idx!=0 and inside_list!=[]:\n","          lista.append(inside_list)\n","          inside_list = []\n","    if line[-4:]!='jpg\\n' and len(line)>15:\n","       num_list = line.strip()\n","       num_str = num_list.split()\n","       x1 = int(num_str[0])\n","       y1 = int(num_str[1])\n","       x2 = int(num_str[0]) + int(num_str[2])\n","       y2 = int(num_str[1]) + int(num_str[3])\n","       if num_str[2]!='0' and num_str[3]!='0' and num_str[7]!='1' and (x2-x1)>20 and (x2-x1)<1200 and (y2-y1)>20 and (y2-y1)<1200:\n","          array_int = [1, x1, y1, x2, y2]\n","          all_boxes.append(array_int[1:])\n","          inside_list.append(array_int)\n","       else:\n","          triger=1\n","    if idx == 185183:\n","       if inside_list!=[] and triger==0:\n","          lista.append(inside_list)\n","       elif inside_list==[] and triger==1:\n","          names.pop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbtuMhGXm7Qi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1689374158621,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"arF69zm-k-a3","outputId":"463e9273-bffc-459a-bd24-41109ff05ee7"},"outputs":[{"output_type":"stream","name":"stdout","text":["9074\n"]}],"source":["#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","dataset = LoadDataSet(annotation_path=lista, images_path=names, transforms=None)\n","data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, drop_last=True)\n"]},{"cell_type":"markdown","source":["k mean clustring\n"],"metadata":{"id":"ctzXjdU_wSAn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryg51GiL-1j6"},"outputs":[],"source":["\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2 as cv2\n","import torch\n","import copy\n","boxes = torch.tensor([[10,10,90,40], [80, 70, 120, 190], [250, 5, 300, 250], [70, 90, 350, 500],\n","                  [290, 200, 430, 350], [390, 310, 520, 470],  [320, 230, 370, 310], [15, 150, 260, 550], [70, 50, 500, 530],\n","                  [120, 300, 250, 450], [60, 50, 270, 280]]).to(float)\n","\n","centroids = torch.tensor([[100, 150, 160, 210], [50,50,205,347],\n","                      [10, 5, 416, 407], [0, 0, 400, 200],[0, 0,140, 500]]).to(float)\n","\n","\n","\n","img = np.random.randint(240, 255, (416,416,3)).astype('float32')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxQlm7oZC1yM"},"outputs":[],"source":["import torch\n","bounding_boxes = torch.tensor(all_boxes).to(float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EynOGsL9Rie9"},"outputs":[],"source":["new_boxes = copy.deepcopy(boxes_for_anchors)\n","for i, box in enumerate(new_boxes):\n","  new_boxes[i] = [0,0, box[2]-box[0], box[3] - box[1]]\n","\n","new_boxes = torch.tensor(new_boxes).to(torch.float32)\n","\n","new_centroids = copy.deepcopy(centroids)\n","for i, box in enumerate(new_centroids):\n","  new_centroids[i] = torch.tensor([0, 0, box[2]-box[0], box[3]-box[1]]).to(float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3acEre0-2ng"},"outputs":[],"source":["def calc_iou_anchors(boxes, centroids):\n","  boxes_area = (boxes[:,2] - boxes[:,0]) * (boxes[:,3] - boxes[:,1])\n","  centroids_area = (centroids[:,2] - centroids[:,0]) * (centroids[:,3] - centroids[:,1])\n","\n","  upper_left_intersection = torch.max(boxes[:,:2].unsqueeze(1), centroids[:,:2])\n","  lower_right_intersection = torch.min(boxes[:, 2:].unsqueeze(1), centroids[:,2:])\n","\n","  intersection_dimension = (lower_right_intersection - upper_left_intersection).clamp(min=0)\n","  intersection_area = intersection_dimension[:,:,0] * intersection_dimension[:,:,1]\n","  union_area = boxes_area.unsqueeze(1) + centroids_area - intersection_area\n","\n","  return intersection_area/union_area\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdQmhc13qKKA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5PEiUlBuOTs1"},"outputs":[],"source":["def calc_closest(new_boxes, new_centroids):\n","  iou = calc_iou_anchors(new_boxes, new_centroids)\n","\n","  distance = 1 - iou\n","  _, indices = torch.min(distance, dim=1)\n","\n","  closets = []\n","  for i in range(len(new_centroids)):\n","      closets.append(torch.where(indices == i)[0])\n","\n","  return closets\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N85Qbz0gWy_m"},"outputs":[],"source":["while True:\n","  check_centroid = copy.deepcopy(new_centroids)\n","  for i  in range(len(new_centroids)):\n","      closets = calc_closest(new_boxes, new_centroids)\n","      new_centroids[i] = torch.mean(new_boxes[closets[i]], dim=0)\n","  print(0)\n","  if new_centroids.equal(check_centroid):\n","     print(check_centroid)\n","     print(new_centroids)\n","     break\n"]},{"cell_type":"markdown","source":["creating anchors after the k-mean clustring for the cetroids\n"],"metadata":{"id":"GetDYccxwZkb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gqOs24Ysu0X"},"outputs":[],"source":["import torch\n","new_centroids = torch.tensor([[  0.0000,   0.0000,  16.0557,  27.2696],\n","        [  0.0000,   0.0000,  30.4325,  49.2445],\n","        [  0.0000,   0.0000, 180.2798, 241.7805],\n","        [  0.0000,   0.0000,  53.1910,  86.6925],\n","        [  0.0000,   0.0000,  97.2692, 146.4186]], dtype=torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMk2_dlesvTz"},"outputs":[],"source":["'''\n","for box in new_boxes:\n","  cv2.rectangle(img,  ( int(box[0]), int(box[1]) ), ( int(box[2]), int(box[3]) ),\n","                (0,0,255), 1)\n","'''\n","all_anchors = []\n","\n","for i in range(13):\n","  for j in range(13):\n","    anchors_per_cell = []\n","    #if i == 6 and j==6:\n","    for box in new_centroids:\n","        '''\n","        cv2.rectangle(img, ( (i)*32 + 16 - int( (box[2]-box[0])/2  ) , (j)*32 + 16-int( (box[3]- box[1])/2 ) ),\n","               ((i)*32 +16 + int( (box[2] - box[0])/2 ), (j)*32 +16+ int( (box[3] - box[1])/2 )), (0,255,0),2)\n","        print([(i)*32 + 16 -int((box[2]-box[0])/2), (j) * 32 + 16 - int((box[3] - box[1])/2),\n","             (i)*32 +16 + int((box[2]-box[0])/2), (j)*32 +16 +int((box[3]-box[1])/2)])\n","        '''\n","        anchors_per_cell.append([ (i)*32 + 16 -int((box[2]-box[0])/2), (j)*32 + 16 - int((box[3]-box[1])/2),\n","                               (i)*32 + 16 + int((box[2]-box[0])/2), (j)*32 + 16+ int((box[3]-box[1])/2)])\n","\n","\n","    all_anchors.append(anchors_per_cell)\n","    #if j==12 and i==12:\n","      #break\n","  #if j==12 and i==12:\n","    #break\n","\n","#plt.imshow(img)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1689371080989,"user":{"displayName":"Hemo Hamo","userId":"14653600239742849398"},"user_tz":-120},"id":"UkQqtMPmcKXE","outputId":"9c5c7566-9015-4aa1-d01b-8e6bd6e77ab2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["169"]},"metadata":{},"execution_count":121}],"source":["len(all_anchors)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}